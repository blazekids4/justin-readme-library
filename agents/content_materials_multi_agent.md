# Media Material Intelligence Platform - POC

An AI-powered platform for analyzing, querying, and extracting insights from large-scale media material databases using concurrent multi-agent processing and intelligent reasoning.

## ğŸ¯ Summary

This POC demonstrates advanced AI agent orchestration for media operations, transforming how content teams interact with material databases. Using Microsoft's Agent Framework, the platform processes 50K+ media records through concurrent AI agents that work in parallel to answer complex queries, analyze distributions, and uncover insightsâ€”all in natural language.

**Think of it as:** Ask your media database questions in plain English and get comprehensive, data-driven answers in minutes instead of days.

## ğŸ’¡ Value Proposition

### For Media Operations Teams

- **âš¡ Faster Analysis**: Concurrent agent processing vs traditional sequential methods
- **ğŸ§  Natural Language Querying**: No SQL, no codingâ€”just ask your questions
- **ğŸ“Š Instant Insights**: Automatic pattern discovery, distribution analysis, and anomaly detection
- **ğŸ¯ Comprehensive Answers**: Get statistics, breakdowns, and recommendations in one response

### For Technical Directors

- **ğŸ—ï¸ Production-Ready Architecture**: Built on Microsoft Agent Framework with proven patterns
- **ğŸ“ˆ Scalable Design**: Handles datasets of any size through intelligent chunking
- **ğŸ”„ Adaptive Processing**: AI determines optimal data partitioning strategy
- **âš™ï¸ Configurable & Extensible**: Easy to customize for specific use cases

### Business Impact

- **Time Savings**: Hours â†’ Minutes for inventory analysis and reporting
- **Better Decisions**: Data-driven insights about material distribution and availability
- **Operational Efficiency**: Reduce manual database queries and spreadsheet analysis
- **Cost Effective**: Leverage existing Azure AI infrastructure

## ğŸš€ Key Features

### 1. **Concurrent Multi-Agent Processing**

- Multiple AI agents analyze data chunks in parallel
- Automatic fan-out/fan-in orchestration using `ConcurrentBuilder`
- Graceful error handling with agent isolation
- Significantly faster than sequential processing for large datasets

### 2. **Intelligent Data Chunking**

AI-driven analysis determines optimal chunking strategy:

- **Count-based**: Equal-sized chunks for uniform data
- **Title-based**: Group by content title for title queries
- **MediaType-based**: Group by Audio/Video/Timed Text
- **Library-based**: Group by Servicing/Archive classification
- **Hybrid**: Multi-dimensional strategies for complex datasets

### 3. **AI Reasoning & Synthesis**

Master reasoning agent that:

- Synthesizes findings from all chunk analyzers
- Answers queries with aggregated statistics
- Identifies patterns and anomalies automatically
- Provides recommendations based on data analysis

### 4. **Comprehensive Query Support**

Natural language queries across multiple dimensions:

- Inventory counts and distributions
- Status tracking (Available, Processing, etc.)
- Language and format analysis
- Title and library breakdowns
- Cross-cutting filters (e.g., "English 5.1 audio in Servicing")

### 5. **Rich Output Formats**

- **Markdown reports**: Human-readable analysis with tables and insights
- **JSON data**: Structured results for downstream processing
- **Executive summaries**: High-level findings for stakeholders
- **Processing metadata**: Performance metrics and confidence scores

## ğŸ“‹ Sample Queries & Use Cases

### Inventory Management

```
Query: "What materials are listed in the database? Provide a breakdown by media type."
Result: Complete inventory with Audio, Video, and Timed Text distribution analysis
Use Case: Quarterly inventory audits and reporting
```

### Technical Format Analysis

```
Query: "How many 5.1 audio layouts do we have? What about Stereo?"
Result: Distribution of audio layouts with 5.1, Stereo, and other format percentages
Use Case: Technical requirements planning for distribution
```

### Content Analysis

```
Query: "Which content items have the most materials? Show top 10."
Result: Ranked list with material counts per content item
Use Case: Content prioritization and archival decisions
```

### Language Distribution

```
Query: "Show me the distribution of materials by language."
Result: Language breakdown with primary languages and percentages
Use Case: Localization planning and resource allocation
```

### Availability Status

```
Query: "How many materials are Available vs Processing vs other statuses?"
Result: Status distribution across Available, Processing, and Archived states
Use Case: Operations monitoring and capacity planning
```

### Cross-cutting Filters

```
Query: "English audio files with Dialog purpose in primary library that are Available"
Result: Materials matching all criteria with detailed breakdown
Use Case: Specific material retrieval for delivery projects
```

### Library Operations

```
Query: "Compare materials in primary vs archive libraries"
Result: Library distribution with trend analysis
Use Case: Storage optimization and migration planning
```

## ğŸ—ï¸ How It Works

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Natural Language Query               â”‚
â”‚           "What audio materials are available?"                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 1: Data Structure Analysis (JSONAnalyzerExec)            â”‚
â”‚  â€¢ Analyzes 50K+ material records                                â”‚
â”‚  â€¢ Identifies fields: mediaType, language, status, etc.          â”‚
â”‚  â€¢ Determines optimal chunking: count/title/mediaType/library    â”‚
â”‚  â€¢ Outputs: Strategy + Chunk size + Rationale                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 2: Concurrent Chunk Processing (ChunkAnalyzerExec Ã— N)   â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Chunk 1   â”‚  â”‚  Chunk 2   â”‚  â”‚  Chunk 3   â”‚  â”‚  Chunk N â”‚ â”‚
â”‚  â”‚  Agent     â”‚  â”‚  Agent     â”‚  â”‚  Agent     â”‚  â”‚  Agent   â”‚ â”‚
â”‚  â”‚  Analyzes  â”‚  â”‚  Analyzes  â”‚  â”‚  Analyzes  â”‚  â”‚  Analyzesâ”‚ â”‚
â”‚  â”‚  7K recordsâ”‚  â”‚  7K recordsâ”‚  â”‚  7K recordsâ”‚  â”‚  7K recs â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        â”‚                â”‚                â”‚               â”‚       â”‚
â”‚   Statistics      Statistics       Statistics      Statistics   â”‚
â”‚   Patterns        Patterns         Patterns        Patterns     â”‚
â”‚   Findings        Findings         Findings        Findings     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
         â”‚                â”‚                â”‚               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 3: Reasoning & Synthesis (ReasoningAgentExec)            â”‚
â”‚  â€¢ Aggregates all chunk statistics                               â”‚
â”‚  â€¢ Cross-validates findings                                      â”‚
â”‚  â€¢ Answers user query with comprehensive data                    â”‚
â”‚  â€¢ Generates insights and recommendations                        â”‚
â”‚  â€¢ Produces markdown + JSON outputs                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Comprehensive Answer                          â”‚
â”‚  â€¢ Direct answer to query                                        â”‚
â”‚  â€¢ Supporting statistics from 50K+ records                       â”‚
â”‚  â€¢ Distribution breakdowns and charts                            â”‚
â”‚  â€¢ Key insights and patterns                                     â”‚
â”‚  â€¢ Recommendations for action                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Processing Flow

1. **Load Data**: Read media material JSON (56,875 records â‰ˆ 50MB)
2. **Analyze Structure**: AI examines fields, cardinality, distribution
3. **Determine Strategy**: AI recommends chunking approach (e.g., 8 chunks Ã— 7K records)
4. **Create Chunks**: Split data according to optimal strategy
5. **Parallel Processing**: Launch N concurrent chunk analyzer agents
6. **Extract Statistics**: Each agent analyzes its chunk for patterns
7. **Aggregate Results**: Collect findings from all agents
8. **Reason & Synthesize**: Master agent combines insights and answers query
9. **Generate Output**: Create markdown reports, JSON data, summaries

### Technical Implementation

**Framework**: Microsoft Agent Framework

- `Executor` base classes with `@handler` decorators
- `AgentExecutorRequest` / `AgentExecutorResponse` messaging
- `WorkflowContext` for asynchronous output handling
- `ConcurrentBuilder` for parallel orchestration
- `ChatAgent` with custom instructions and tools

**AI Models**: Azure OpenAI (GPT-4.1)

- Structured reasoning for data analysis
- Natural language understanding for queries
- Pattern recognition and anomaly detection

**Performance**:

- **Concurrent Processing**: Suitable for large datasets (50K+ records)
- **Sequential Processing**: Alternative approach for smaller datasets
- **Memory Usage**: Efficient in-memory processing
- **Scalability**: Configurable chunk size for datasets of any size

## ğŸ› ï¸ Installation & Setup

### Prerequisites

- Python 3.9+
- Azure OpenAI or Azure AI Foundry access
- Azure CLI (for authentication)

### 1. Clone Repository

```bash
git clone https://github.com/your-org/media-material-intelligence-poc.git
cd media-material-intelligence-poc
```

### 2. Create Virtual Environment

```bash
python -m venv venv
.\venv\Scripts\Activate.ps1  # Windows PowerShell
# source venv/bin/activate    # Linux/Mac
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure Environment

Create `.env` file in project root:

```bash
AZURE_AI_PROJECT_ENDPOINT=https://your-project.openai.azure.com
AZURE_AI_MODEL_DEPLOYMENT_NAME=gpt-4.1
```

### 5. Authenticate with Azure

```bash
az login
```

## ğŸš€ Usage

### Quick Start - Concurrent Material Analyzer

```bash
cd scripts/concurrent-agents
python concurrent_material_analyzer.py
```

**Interactive Prompt:**

```
ğŸ” Enter your query (or press Enter for example query 1):
> What materials are listed in the database? Provide a breakdown by media type.
```

**Output Location:**

```
output/material_analysis_20251029_143022/
  â”œâ”€â”€ final_answer.md           # Comprehensive answer
  â”œâ”€â”€ chunk_analyses.json       # Detailed chunk data
  â””â”€â”€ executive_summary.md      # Processing summary
```

### Command Line Options

```bash
# Use default example query
python concurrent_material_analyzer.py

# Interactive mode (default)
python concurrent_material_analyzer.py --interactive

# Programmatic query
python concurrent_material_analyzer.py --query "How many audio files?"
```

## ğŸ“Š Project Structure

```
media-material-intelligence-poc/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ material-data.json                    # Material records dataset
â”‚   â”œâ”€â”€ rendition-data.json                   # Rendition metadata
â”‚   â””â”€â”€ cache/                                # Processing cache
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ concurrent-agents/
â”‚   â”‚   â”œâ”€â”€ concurrent_material_analyzer.py   # Main concurrent analyzer
â”‚   â”‚   â”œâ”€â”€ concurrent_framework_revenue_retrieval.py  # Reference implementation
â”‚   â”‚   â”œâ”€â”€ README_MATERIAL_ANALYZER.md       # Full documentation
â”‚   â”‚   â””â”€â”€ QUICK_START.md                    # Quick reference guide
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ extract_schema_llm.py             # Schema extraction
â”‚   â”‚   â”œâ”€â”€ json_to_excel_csv.py              # Data export utilities
â”‚   â”‚   â””â”€â”€ process_json_for_embedding.py     # Embedding prep
â”‚   â”œâ”€â”€ foundry-data-agent/
â”‚   â”‚   â””â”€â”€ single_foundry_agent.py           # Single-agent implementation
â”‚   â””â”€â”€ analyze_material_llm.py               # Material analysis utilities
â”œâ”€â”€ output/
â”‚   â””â”€â”€ material_analysis_*/                   # Analysis results by timestamp
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ user_1.J2                             # Jinja2 prompt templates
â”œâ”€â”€ requirements.txt                          # Python dependencies
â”œâ”€â”€ .env                                      # Environment configuration
â””â”€â”€ README.md                                 # This file
```

## ğŸ“ Key Components

### Concurrent Material Analyzer

**Location**: `scripts/concurrent-agents/concurrent_material_analyzer.py`

The flagship implementation demonstrating:

- AI-driven chunking strategy selection
- Concurrent multi-agent processing
- Master reasoning agent for synthesis
- Natural language query interface

**Documentation**:

- `README_MATERIAL_ANALYZER.md` - Complete technical docs
- `QUICK_START.md` - Quick reference for users

### Schema Extraction

**Location**: `scripts/experiments/extract_schema_llm.py`

Analyzes JSON structure to extract:

- Field types and distributions
- Nullable fields and occurrence rates
- Array structures and examples
- Data quality metrics

### Data Export Utilities

**Location**: `scripts/experiments/json_to_excel_csv.py`

Convert JSON analysis results to:

- Excel spreadsheets with formatting
- CSV files for downstream processing
- Custom report templates

## ğŸ”§ Configuration

### Chunking Strategy

Adjust in `concurrent_material_analyzer.py`:

```python
# Target number of chunks (default: 8)
target_chunks = 8  # Line 415

# Minimum chunk size (default: 100)
chunk_size = max(100, total_records // target_chunks)
```

### Agent Instructions

Customize agent behavior:

- **JSONAnalyzerExec**: Modify chunking analysis instructions
- **ChunkAnalyzerExec**: Customize statistics extraction focus
- **ReasoningAgentExec**: Adjust synthesis and query answering approach

### Model Configuration

Update `.env`:

```bash
# Use different model
AZURE_AI_MODEL_DEPLOYMENT_NAME=gpt-4.1

# Adjust timeout
AGENT_TIMEOUT_SECONDS=120
```

## ğŸ“ˆ Performance Characteristics

### Sample Dataset Performance

*Note: Performance metrics are estimates based on typical dataset characteristics and should be validated in your specific environment.*

| Processing Mode        | Relative Speed | Throughput       | Memory Usage |
| ---------------------- | -------------- | ---------------- | ------------ |
| Concurrent (8 agents)  | Fast           | High             | Moderate     |
| Concurrent (12 agents) | Faster         | Higher           | Moderate     |
| Sequential             | Baseline       | Lower            | Lower        |

### Query Response Characteristics

| Query Type               | Complexity | Relative Response Time |
| ------------------------ | ---------- | ---------------------- |
| Simple count             | Low        | Fast                   |
| Distribution analysis    | Medium     | Moderate               |
| Multi-dimensional filter | High       | Slower                 |
| Top-N ranking            | High       | Slower                 |

_Response times include full data processing, reasoning, and output generation_

**Performance Factors**: Actual performance depends on dataset size, complexity, network conditions, Azure region, current API load, and specific query patterns.

## ğŸ¯ Use Cases by Role

### Content Operations Manager

- **Daily**: Material availability monitoring
- **Weekly**: Status distribution reports for team meetings
- **Monthly**: Inventory audits and trend analysis
- **Quarterly**: Cross-library migration planning

### Technical Director

- **Project Planning**: Format and layout distribution for delivery specs
- **Resource Allocation**: Language and localization capacity planning
- **Quality Assurance**: Anomaly detection in material metadata
- **Optimization**: Storage and archival strategy based on usage patterns

### Media Librarian

- **Cataloging**: Title-based material organization
- **Discovery**: Finding specific materials by multiple criteria
- **Reporting**: Library statistics for stakeholder updates
- **Compliance**: Verifying material completeness for audits

### Data Analyst

- **Exploration**: Understanding dataset characteristics
- **Validation**: Cross-checking material counts and distributions
- **Integration**: Extracting structured data for BI tools
- **Automation**: Building repeatable analysis workflows

## ğŸ”¬ Advanced Features

### Custom Chunking Strategies

Implement domain-specific chunking:

```python
def custom_chunking_strategy(data, metadata):
    """Chunk by release date and content type"""
    # Your custom logic here
    return chunks
```

### Query Templates

Pre-built templates for common queries:

```python
QUERY_TEMPLATES = {
    "inventory_audit": "What materials are listed? Breakdown by type, status, and library.",
    "format_analysis": "Analyze audio layouts and video formats. Show distribution.",
    "language_report": "Language distribution across all materials. Include percentages.",
}
```

### Output Customization

Generate custom report formats:

```python
async def generate_custom_report(results, template="executive"):
    # Use Jinja2 templates for custom formatting
    # Export to PDF, Excel, or custom formats
    pass
```

## ğŸ› Troubleshooting

### Common Issues

**"AZURE_AI_PROJECT_ENDPOINT not set"**

```bash
# Ensure .env file exists with correct values
# Verify environment variables are loaded
python -c "import os; from dotenv import load_dotenv; load_dotenv(); print(os.getenv('AZURE_AI_PROJECT_ENDPOINT'))"
```

**"Authentication failed"**

```bash
# Re-authenticate with Azure CLI
az login
az account show
```

**"Memory error with large datasets"**

```python
# Reduce chunk count to process smaller portions
target_chunks = 12  # Instead of 8, creates smaller chunks
```

**"Agent timeout"**

```bash
# Increase timeout in .env
AGENT_TIMEOUT_SECONDS=300
```

## ğŸ¤ Contributing

This is a proof-of-concept project. For enhancements:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/enhancement`)
3. Make your changes
4. Test thoroughly with sample queries
5. Commit changes (`git commit -am 'Add enhancement'`)
6. Push to branch (`git push origin feature/enhancement`)
7. Create Pull Request

## ğŸ“š Documentation

- **Quick Start**: `scripts/concurrent-agents/QUICK_START.md`
- **Full Documentation**: `scripts/concurrent-agents/README_MATERIAL_ANALYZER.md`
- **Script Reference**: `scripts/README.md`
- **Agent Framework**: [Microsoft Agent Framework Docs](https://docs.microsoft.com/azure/ai)

## ğŸ” Security & Compliance

- Azure AD authentication via Azure CLI
- No API keys stored in code
- Environment variables for configuration
- Secure credential management with Azure Identity
- Data processed in memory, not persisted by default

## ğŸ“ License

This is a proof-of-concept project for demonstration purposes.

## ğŸ™ Acknowledgments

- **Microsoft Agent Framework**: Concurrent agent orchestration
- **Azure OpenAI**: AI reasoning and natural language processing
- **Azure AI Foundry**: Integrated AI development platform

## ğŸ“ Support

For questions or issues:

- Review documentation in `scripts/concurrent-agents/`
- Check troubleshooting section above
- Open an issue in the repository

---

**Built with** â¤ï¸ **using Microsoft Agent Framework and Azure AI**

**Last Updated**: October 29, 2025
**Version**: 1.0.0
**Status**: Proof of Concept - Production Ready
